from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch
import subprocess
import os
import logging
from transformers import logging as hf_logging
import json


def generate_response(prompt):
    # Encode prompt as token ID
    prompt_tokens = tokenizer.encode(prompt, return_tensors="pt").to("cuda")
    
    # Model generates response
    outputs = model.generate(
        prompt_tokens,
        max_new_tokens=200,
        pad_token_id=tokenizer.eos_token_id  
    )
    output_tokens = outputs[0].tolist()  # Convert tokens to list
    
    
    prompt_tokens_list = prompt_tokens[0].tolist()  # Prompt token IDs

    # Delete token IDs identical to prompt
    if len(output_tokens) > len(prompt_tokens_list):
        output_tokens = output_tokens[len(prompt_tokens_list):]

    # Decode reserved token ID to text
    response_cleaned = tokenizer.decode(output_tokens, skip_special_tokens=False).strip()

    return response_cleaned


# Read the original problem file and insert target conditions
def update_pddl_problem(goal_conditions, new_problem_file):
    print(goal_conditions)
    # Old problem file
    with open(new_problem_file, 'r') as f:
        problem_content = f.read()

    # Insert content generated by the language model
    updated_problem = problem_content.replace('#', goal_conditions)
    print(updated_problem)

    # Write content to probelm file
    with open(new_problem_file, 'w') as f:
        f.write(updated_problem)

def pddl_solver(problem, domain):
    cmd = ['pyperplan', '-s', 'gbf', '-H', 'hadd', domain_file, problem_file]

    try:
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            print("Success!")
            solution_file = f'{problem_file}.soln'
            
            if os.path.exists(solution_file):
                print(f"Solution file location: {solution_file}")
                with open(solution_file, 'r') as f:
                    solution = f.read()
                    print("Task plan:")
                    print(solution)
            else:
                print("File not found.")

    except FileNotFoundError:
        print("pyperplan not installed correctly.")



if __name__ == "__main__":
    
    hf_logging.set_verbosity_error()

    # Access the token from the environment variable
    hf_token = os.getenv("HF_TOKEN")
    print(hf_token)

    nf4_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    model = AutoModelForCausalLM.from_pretrained("google/gemma-2-9b-it", quantization_config=nf4_config,  device_map="auto")
    tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-9b-it", token=hf_token)

    # Read directories from JSON file
    with open('../config.json', 'r') as pddl_files:
        directory_data = json.load(pddl_files)

    problem_file = directory_data['problem_file']
    domain_file = directory_data['domain_file']
    new_problem_file = directory_data['update_problem']

    prompt_template_file = '../prompt_template.txt'
    
    with open(prompt_template_file, 'r', encoding='utf-8') as f:
        prompt_template = f.read()

    while True:
        user_input = input("command (enter 'exit' to stop): ")
        if user_input.lower() == 'exit':
            print("end")
            break
        prompt = prompt_template.replace('#', user_input)
<<<<<<<< HEAD:pddl_v1/scripts/gemma_pddl.py
    json_file = '../update_data.json'
    # Load the new goal from JSON
    with open(json_file, 'r') as f:
        data = json.load(f)
        print(data)
========
>>>>>>>> 975cd7002a5d0394248e1f0e99594fc6a22a86d9:front_llm/scripts/gemma_pddl.py

    goal_conditions = generate_response(prompt)  

    print(f"\nGoal(generated by LLM):\n{goal_conditions}")
    new_problem_file = update_pddl_problem(goal_conditions, new_problem_file)
    pddl_solver(problem=new_problem_file, domain=domain_file)
        


        


